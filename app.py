"""
Capital Bikeshare Intelligence Dashboard
Enhanced Production Version - Adapted for Hugging Face Spaces
File Structure: data/ and models/ folders
"""

import gradio as gr
import pandas as pd
import numpy as np
import joblib
import folium
from datetime import datetime
import plotly.graph_objects as go
import plotly.express as px
from haversine import haversine, Unit
from pulp import LpMinimize, LpProblem, LpVariable, lpSum, value
import warnings
import os
warnings.filterwarnings('ignore')

# ============================================================================
# CONFIGURATION
# ============================================================================

class Config:
    """Configuration settings"""
    BASE_PATH = os.path.dirname(__file__)
    DATA_PATH = os.path.join(BASE_PATH, 'data')
    MODEL_PATH = os.path.join(BASE_PATH, 'models')
    
    # Optimization parameters
    COST_PER_KM = 0.5
    PENALTY_SHORTAGE = 10.0
    
    # Map settings
    DC_CENTER = [38.9072, -77.0369]
    MAP_ZOOM = 12

# ============================================================================
# DATA LOADING
# ============================================================================

def load_all_data():
    """Load all necessary data and models with error handling"""
    try:
        print("üìÇ Loading data from file structure...")
        
        data = {}
        
        # Load CSVs from data folder
        csv_files = {
            'station_locations': 'station_locations.csv',
            'station_capacity': 'station_capacity.csv',
            'station_clusters': 'station_clusters.csv'
        }
        
        for key, filename in csv_files.items():
            filepath = os.path.join(Config.DATA_PATH, filename)
            data[key] = pd.read_csv(filepath)
            print(f"  ‚úì Loaded {filename} from data/")
        
        # Load distance matrix from data folder
        dist_file = os.path.join(Config.DATA_PATH, 'distance_matrix.npy')
        data['distance_matrix'] = np.load(dist_file)
        print(f"  ‚úì Loaded distance_matrix.npy from data/")
        
        # Load ML models from models folder
        models = {}
        for i in range(5):
            model_file = os.path.join(Config.MODEL_PATH, f'model_cluster_{i}.pkl')
            models[i] = joblib.load(model_file)
            print(f"  ‚úì Loaded model_cluster_{i}.pkl from models/")
        data['models'] = models
        
        # Merge station info
        station_info = data['station_locations'].merge(
            data['station_capacity'], on='station_id', how='left'
        ).merge(
            data['station_clusters'], on='station_id', how='left'
        )
        station_info['estimated_capacity'] = station_info['estimated_capacity'].fillna(20).astype(int)
        data['station_info'] = station_info
        
        print(f"\n‚úÖ All data loaded successfully!")
        print(f"  ‚Ä¢ {len(station_info)} stations")
        print(f"  ‚Ä¢ {len(models)} cluster models")
        
        return data
        
    except Exception as e:
        print(f"‚ùå Error loading data: {e}")
        print(f"Expected structure:")
        print(f"  - data/station_locations.csv")
        print(f"  - data/station_capacity.csv")
        print(f"  - data/station_clusters.csv")
        print(f"  - data/distance_matrix.npy")
        print(f"  - models/model_cluster_0.pkl")
        print(f"  - models/model_cluster_1.pkl")
        print(f"  - models/model_cluster_2.pkl")
        print(f"  - models/model_cluster_3.pkl")
        print(f"  - models/model_cluster_4.pkl")
        raise

# Load data globally
try:
    DATA = load_all_data()
except Exception as e:
    print(f"CRITICAL ERROR: Could not load data.")
    raise

# ============================================================================
# PREDICTION ENGINE
# ============================================================================

def predict_demand(target_datetime, models, station_info, weather_params):
    """
    Predict bike demand for all stations using XGBoost models.
    
    How it works:
    1. Extracts time features (hour, day, month) from target datetime
    2. Creates cyclical encodings (sin/cos) for better pattern capture
    3. Uses weather parameters to adjust predictions
    4. Applies cluster-specific model to each station
    5. Returns predicted demand per station
    """
    try:
        hour = target_datetime.hour
        day_of_week = target_datetime.dayofweek
        month = target_datetime.month
        is_weekday = 1 if day_of_week < 5 else 0
        
        # Cyclical features (converts time to continuous values)
        hour_sin = np.sin(2*np.pi*hour/24)
        hour_cos = np.cos(2*np.pi*hour/24)
        dow_sin = np.sin(2*np.pi*day_of_week/7)
        dow_cos = np.cos(2*np.pi*day_of_week/7)
        month_sin = np.sin(2*np.pi*month/12)
        month_cos = np.cos(2*np.pi*month/12)
        
        predictions = []
        for _, station in station_info.iterrows():
            # Combine all features
            features = {
                'hour': hour, 'day_of_week': day_of_week, 'month': month,
                'is_weekday': is_weekday,
                'hour_sin': hour_sin, 'hour_cos': hour_cos,
                'dow_sin': dow_sin, 'dow_cos': dow_cos,
                'month_sin': month_sin, 'month_cos': month_cos,
                **weather_params
            }
            
            X = pd.DataFrame([features])
            # Use cluster-specific model
            model = models[int(station['cluster_label'])]
            pred = max(0, model.predict(X)[0])
            
            predictions.append({
                'station_id': station['station_id'],
                'station_name': station['station_name'],
                'predicted_demand': round(pred, 2),
                'cluster_label': int(station['cluster_label']),
                'latitude': station['latitude'],
                'longitude': station['longitude'],
                'capacity': int(station['estimated_capacity'])
            })
        
        return pd.DataFrame(predictions)
        
    except Exception as e:
        print(f"Error in prediction: {e}")
        raise

# ============================================================================
# OPTIMIZATION ENGINE
# ============================================================================

def optimize_rebalancing(predictions_df, current_bikes, distance_matrix):
    """
    Optimize bike rebalancing using Mixed Integer Linear Programming.
    
    Objective: Minimize total cost = transport cost + shortage penalties
    
    Decision Variables:
    - X[i,j]: Number of bikes to move from station i to station j
    - shortage[j]: Unmet demand at station j
    
    Constraints:
    1. Can't move more bikes than available at source
    2. Final bikes must meet predicted demand
    3. Can't exceed station capacity
    
    Returns: List of optimal moves and total cost
    """
    try:
        n = len(predictions_df)
        predictions = predictions_df['predicted_demand'].values
        capacities = predictions_df['capacity'].values
        
        # Create optimization problem
        prob = LpProblem("Bike_Rebalancing", LpMinimize)
        
        # Decision variables: bikes to move from i to j
        X = {}
        for i in range(n):
            for j in range(n):
                if i != j:
                    X[i,j] = LpVariable(f"x_{i}_{j}", lowBound=0, cat='Integer')
        
        # Shortage variables: unmet demand at each station
        shortage = {j: LpVariable(f"short_{j}", lowBound=0) for j in range(n)}
        
        # Objective function: minimize transport + shortage costs
        transport_cost = lpSum(
            X[i,j] * distance_matrix[i,j] * Config.COST_PER_KM 
            for i,j in X
        )
        shortage_cost = lpSum(
            shortage[j] * Config.PENALTY_SHORTAGE 
            for j in shortage
        )
        prob += transport_cost + shortage_cost
        
        # Constraint 1: Can't send more bikes than you have
        for i in range(n):
            prob += lpSum(X[i,j] for j in range(n) if i != j) <= current_bikes[i]
        
        # Constraint 2: Meet demand at each station
        for j in range(n):
            inflow = lpSum(X[i,j] for i in range(n) if i != j)
            final_bikes = current_bikes[j] + inflow
            prob += final_bikes + shortage[j] >= predictions[j]
            prob += final_bikes <= capacities[j]
        
        # Solve the optimization problem
        prob.solve()
        
        # Extract results
        moves = []
        for (i,j), var in X.items():
            if var.varValue and var.varValue > 0.5:
                bikes = int(round(var.varValue))
                moves.append({
                    'from_idx': i,
                    'to_idx': j,
                    'from_station_id': predictions_df.iloc[i]['station_id'],
                    'to_station_id': predictions_df.iloc[j]['station_id'],
                    'from_name': predictions_df.iloc[i]['station_name'],
                    'to_name': predictions_df.iloc[j]['station_name'],
                    'bikes': bikes,
                    'distance_km': round(distance_matrix[i,j], 2),
                    'cost': round(distance_matrix[i,j] * Config.COST_PER_KM * bikes, 2),
                    'from_lat': predictions_df.iloc[i]['latitude'],
                    'from_lng': predictions_df.iloc[i]['longitude'],
                    'to_lat': predictions_df.iloc[j]['latitude'],
                    'to_lng': predictions_df.iloc[j]['longitude']
                })
        
        total_cost = round(value(prob.objective), 2) if prob.status == 1 else 0
        
        return pd.DataFrame(moves), total_cost
        
    except Exception as e:
        print(f"Error in optimization: {e}")
        return pd.DataFrame(), 0

# ============================================================================
# VISUALIZATION FUNCTIONS
# ============================================================================

def create_prediction_map(predictions_df, current_bikes):
    """
    Creates an interactive map showing station status.
    
    Color Coding:
    - Red: Critical Deficit (need bikes urgently)
    - Orange: Deficit (need bikes)
    - Blue: Balanced
    - Green: Surplus (have extra bikes)
    """
    predictions_df = predictions_df.copy()
    predictions_df['current_bikes'] = current_bikes
    predictions_df['balance'] = predictions_df['current_bikes'] - predictions_df['predicted_demand']
    
    m = folium.Map(
        location=Config.DC_CENTER,
        zoom_start=Config.MAP_ZOOM,
        tiles='CartoDB positron'
    )
    
    for _, station in predictions_df.iterrows():
        balance = station['balance']
        
        # Determine color and status
        if balance < -3:
            color, status = 'red', 'Critical Deficit'
        elif balance < 0:
            color, status = 'orange', 'Deficit'
        elif balance > 5:
            color, status = 'green', 'Surplus'
        else:
            color, status = 'blue', 'Balanced'
        
        popup_html = f"""
        <div style="font-family: Arial; width: 280px; padding: 10px;">
            <h4 style="margin: 0 0 10px 0; color: #2c3e50; border-bottom: 2px solid {color};">
                {station['station_name'][:40]}
            </h4>
            <table style="width: 100%; font-size: 13px;">
                <tr><td><b>Station ID:</b></td><td>{int(station['station_id'])}</td></tr>
                <tr><td><b>Status:</b></td><td style="color: {color}; font-weight: bold;">{status}</td></tr>
                <tr><td><b>Current Bikes:</b></td><td>{int(station['current_bikes'])}</td></tr>
                <tr><td><b>Predicted Demand:</b></td><td>{station['predicted_demand']:.2f}</td></tr>
                <tr><td><b>Balance:</b></td><td>{balance:.1f}</td></tr>
                <tr><td><b>Capacity:</b></td><td>{int(station['capacity'])}</td></tr>
                <tr><td><b>Cluster:</b></td><td>{int(station['cluster_label'])}</td></tr>
            </table>
        </div>
        """
        
        folium.CircleMarker(
            location=[station['latitude'], station['longitude']],
            radius=8 + abs(balance) * 0.3,
            popup=folium.Popup(popup_html, max_width=300),
            tooltip=station['station_name'][:30],
            color=color,
            fill=True,
            fillColor=color,
            fillOpacity=0.7,
            weight=2
        ).add_to(m)
    
    legend_html = '''
    <div style="position: fixed; bottom: 50px; left: 50px; width: 180px; 
                background-color: white; border: 2px solid grey; z-index: 9999; 
                padding: 10px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.2);">
        <h4 style="margin: 0 0 10px 0; font-size: 14px;">Station Status</h4>
        <div style="font-size: 12px;">
            <p style="margin: 5px 0;"><span style="color: red; font-size: 16px;">‚óè</span> Critical Deficit</p>
            <p style="margin: 5px 0;"><span style="color: orange; font-size: 16px;">‚óè</span> Deficit</p>
            <p style="margin: 5px 0;"><span style="color: blue; font-size: 16px;">‚óè</span> Balanced</p>
            <p style="margin: 5px 0;"><span style="color: green; font-size: 16px;">‚óè</span> Surplus</p>
        </div>
    </div>
    '''
    m.get_root().html.add_child(folium.Element(legend_html))
    
    return m

def create_analytics(predictions_df, moves_df, current_bikes):
    """Creates three analytical charts for insights"""
    predictions_df = predictions_df.copy()
    predictions_df['current_bikes'] = current_bikes
    predictions_df['balance'] = predictions_df['current_bikes'] - predictions_df['predicted_demand']
    
    # Chart 1: Demand by cluster
    cluster_demand = predictions_df.groupby('cluster_label')['predicted_demand'].sum().reset_index()
    fig1 = px.bar(
        cluster_demand,
        x='cluster_label',
        y='predicted_demand',
        title='Predicted Demand by Station Cluster',
        labels={'cluster_label': 'Cluster', 'predicted_demand': 'Total Demand (bikes)'},
        color='predicted_demand',
        color_continuous_scale='Blues'
    )
    fig1.update_layout(showlegend=False, height=400)
    
    # Chart 2: Balance distribution
    fig2 = go.Figure()
    fig2.add_trace(go.Histogram(
        x=predictions_df['balance'],
        nbinsx=30,
        marker_color='#3498db',
        name='Station Balance'
    ))
    fig2.add_vline(x=0, line_dash="dash", line_color="red", 
                   annotation_text="Perfect Balance")
    fig2.update_layout(
        title='Station Balance Distribution',
        xaxis_title='Balance (Current - Predicted)',
        yaxis_title='Number of Stations',
        height=400
    )
    
    # Chart 3: Move analysis
    if len(moves_df) > 0:
        fig3 = px.scatter(
            moves_df,
            x='distance_km',
            y='bikes',
            title='Rebalancing Moves: Distance vs Bikes',
            labels={'distance_km': 'Distance (km)', 'bikes': 'Bikes to Move'},
            color='cost',
            size='bikes',
            hover_data=['from_name', 'to_name'],
            color_continuous_scale='Viridis',
            height=400
        )
    else:
        fig3 = go.Figure()
        fig3.add_annotation(
            text="‚úÖ No moves required - system is balanced!",
            xref="paper", yref="paper",
            x=0.5, y=0.5,
            showarrow=False,
            font=dict(size=18, color="green")
        )
        fig3.update_layout(title='Rebalancing Moves', height=400)
    
    return fig1, fig2, fig3

def generate_insights(predictions_df, moves_df, current_bikes, total_cost):
    """Generates text-based insights from the simulation"""
    predictions_df = predictions_df.copy()
    predictions_df['current_bikes'] = current_bikes
    predictions_df['balance'] = predictions_df['current_bikes'] - predictions_df['predicted_demand']
    
    insights = []
    
    total_demand = predictions_df['predicted_demand'].sum()
    total_supply = predictions_df['current_bikes'].sum()
    
    insights.append("## üìä DEMAND OVERVIEW")
    insights.append(f"**Total Predicted Demand:** {int(round(total_demand))} bikes")
    insights.append(f"**Total Current Supply:** {int(total_supply)} bikes")
    insights.append(f"**System Balance:** {int(total_supply - total_demand):+d} bikes")
    
    deficit_count = (predictions_df['balance'] < 0).sum()
    surplus_count = (predictions_df['balance'] > 5).sum()
    balanced_count = len(predictions_df) - deficit_count - surplus_count
    
    insights.append("\n## üöâ STATION STATUS")
    insights.append(f"üî¥ **Deficit Stations:** {deficit_count} ({deficit_count/len(predictions_df)*100:.1f}%)")
    insights.append(f"üü¢ **Surplus Stations:** {surplus_count} ({surplus_count/len(predictions_df)*100:.1f}%)")
    insights.append(f"üîµ **Balanced Stations:** {balanced_count} ({balanced_count/len(predictions_df)*100:.1f}%)")
    
    cluster_demand = predictions_df.groupby('cluster_label')['predicted_demand'].sum()
    busiest = cluster_demand.idxmax()
    insights.append(f"\nüéØ **Busiest Cluster:** Cluster {int(busiest)} ({int(round(cluster_demand[busiest]))} bikes)")
    
    insights.append("\n## üöö REBALANCING OPERATIONS")
    if len(moves_df) > 0:
        insights.append(f"**Total Moves Required:** {len(moves_df)}")
        insights.append(f"**Bikes to Relocate:** {int(moves_df['bikes'].sum())}")
        insights.append(f"**Average Distance:** {moves_df['distance_km'].mean():.2f} km")
        insights.append(f"**Total Cost:** ${total_cost:.2f}")
        
        # Management suggestion
        if len(moves_df) <= 10 and moves_df['bikes'].sum() <= 50:
            insights.append("\n‚úÖ **Management Recommendation:** System is manageable without immediate rebalancing.")
            insights.append("The current distribution can handle predicted demand with minor imbalances.")
        elif len(moves_df) <= 30:
            insights.append("\n‚ö†Ô∏è **Management Recommendation:** Rebalancing is recommended but not critical.")
            insights.append("Consider scheduling moves during off-peak hours to optimize operations.")
        else:
            insights.append("\nüö® **Management Recommendation:** Rebalancing is strongly recommended!")
            insights.append("Multiple deficit stations require immediate attention to meet demand.")
        
        if not moves_df.empty:
            max_move = moves_df.loc[moves_df['bikes'].idxmax()]
            insights.append(f"\nüî• **Largest Move:** {int(max_move['bikes'])} bikes")
            insights.append(f"    FROM: {max_move['from_name'][:40]}")
            insights.append(f"    TO: {max_move['to_name'][:40]}")
    else:
        insights.append("‚úÖ **No rebalancing needed** - system is optimally balanced!")
        insights.append("\nüí° **Management Recommendation:** Current bike distribution is excellent.")
        insights.append("No moves required - all stations can meet predicted demand with current inventory.")
    
    return "\n".join(insights)

# ============================================================================
# MAIN SIMULATION FUNCTION
# ============================================================================

# *** CHANGE 1: Updated function signature ***
def run_simulation(selected_date, selected_time, total_bikes, temp, feelslike, humidity, 
                   windspeed, precip, cloudcover, solarradiation, solarenergy, uvindex):
    """
    Main function that runs the complete simulation pipeline:
    1. Parse inputs
    2. Predict demand using ML models
    3. Optimize rebalancing using MILP
    4. Generate visualizations
    5. Create insights
    """
    try:
        target_datetime = pd.to_datetime(f"{selected_date} {selected_time}")
        
        weather_params = {
            'temp': temp,
            'feelslike': feelslike,
            'humidity': humidity,
            'windspeed': windspeed,
            'precip': precip,
            'cloudcover': cloudcover,
            'solarradiation': solarradiation,
            'solarenergy': solarenergy,
            'uvindex': uvindex
        }
        
        predictions_df = predict_demand(
            target_datetime,
            DATA['models'],
            DATA['station_info'],
            weather_params
        )
        
        # *** CHANGE 2: Logic to distribute total bikes ***
        n_stations = len(predictions_df)
        # Calculate base bikes per station and the remainder
        base_bikes = int(total_bikes) // n_stations
        remainder = int(total_bikes) % n_stations
        
        # Create an array with the base number
        current_bikes = np.full(n_stations, base_bikes)
        
        # Distribute the remainder bikes to the first 'remainder' stations
        current_bikes[:remainder] += 1
        # This creates an even distribution, e.g., [11, 11, 10, 10, 10]
        
        moves_df, total_cost = optimize_rebalancing(
            predictions_df,
            current_bikes,
            DATA['distance_matrix']
        )
        
        pred_map = create_prediction_map(predictions_df, current_bikes)
        
        # Save map with explicit height
        pred_map_html = pred_map._repr_html_()
        
        # Wrap in container with fixed height
        pred_html = f'<div style="width: 100%; height: 600px;">{pred_map_html}</div>'
        
        fig1, fig2, fig3 = create_analytics(predictions_df, moves_df, current_bikes)
        insights = generate_insights(predictions_df, moves_df, current_bikes, total_cost)
        
        # *** CHANGE 3: Updated summary text ***
        summary = f"""
## üìã Simulation Summary

**Date & Time:** {target_datetime.strftime('%A, %B %d, %Y at %I:%M %p')}

**Bike Availability:** {int(total_bikes)} total bikes (Avg: {int(total_bikes) / n_stations:.1f} per station)

**Weather Conditions:**
- Temperature: {temp}¬∞C (Feels like: {feelslike}¬∞C)
- Humidity: {humidity}% | Wind: {windspeed} km/h
- Precipitation: {precip} mm | Cloud Cover: {cloudcover}%
- Solar Radiation: {solarradiation} W/m¬≤ | UV Index: {uvindex}

**Key Metrics:**
- Total Moves Required: **{len(moves_df)}**
- Total Cost: **${total_cost:.2f}**
- Bikes to Relocate: **{int(moves_df['bikes'].sum()) if len(moves_df) > 0 else 0}**
        """
        
        if len(moves_df) > 0:
            csv_content = moves_df.to_csv(index=False)
            csv_filename = f"rebalancing_plan_{target_datetime.strftime('%Y%m%d_%H%M')}.csv"
        else:
            csv_content = "No moves required - system is balanced!\n"
            csv_filename = "no_moves_needed.txt"
        
        with open(csv_filename, 'w') as f:
            f.write(csv_content)
        
        return (
            pred_html,
            fig1,
            fig2,
            fig3,
            insights,
            summary,
            csv_filename
        )
        
    except Exception as e:
        error_msg = f"‚ùå Simulation failed: {str(e)}"
        print(error_msg)
        return (
            "<h3>Error loading map</h3>",
            go.Figure(),
            go.Figure(),
            go.Figure(),
            error_msg,
            error_msg,
            None
        )

# ============================================================================
# GRADIO INTERFACE
# ============================================================================

custom_css = """
.gradio-container {
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif !important;
}
body {
    background-color: #0b0f19 !important;
}
.contain {
    background-color: #0b0f19 !important;
}
#component-0, #component-1, #component-2, #component-3, #component-4 {
    background-color: #0b0f19 !important;
}
.markdown-text, .prose {
    color: #ffffff !important;
}
h1, h2, h3, h4, h5, h6 {
    color: #ffffff !important;
}
p, li, td, th, span, label {
    color: #e5e7eb !important;
}
.gr-box {
    background-color: #1f2937 !important;
    border-color: #374151 !important;
}
.gr-input, .gr-text-input {
    background-color: #1f2937 !important;
    color: #ffffff !important;
}
iframe {
    background-color: #ffffff !important;
    border-radius: 8px;
    min-height: 600px !important;
    height: 600px !important;
}
"""

with gr.Blocks(css=custom_css, title="Capital Bikeshare Intelligence", theme=gr.themes.Base()) as demo:
    
    gr.Markdown("""
    # üö¥ Capital Bikeshare Intelligence Dashboard
    ### Demand Prediction & Operations Optimization
    
    This dashboard uses XGBoost with K-Means clustering and does Operations Optimization (Mixed Integer Linear Programming) to predict bike demand and optimize rebalancing operations.
    """)
    
    # gr.Markdown("""
    # ---
    # ## üìñ How It Works
    
    # ### 1Ô∏è‚É£ **Demand Prediction** (Machine Learning)
    # - Uses **5 cluster-specific XGBoost models** trained on historical data
    # - Each station belongs to a cluster (Commuter Hub, Leisure, High Traffic, etc.)
    # - Factors: Time (hour, day, month), Weather (9 parameters), Station characteristics
    # - Output: Predicted bikes needed per station for the next hour
    
    # ### 2Ô∏è‚É£ **Rebalancing Optimization** (Operations Research)
    # - Solves Mixed Integer Linear Programming problem
    # - Objective: Minimize (Transport Cost + Shortage Penalties)
    # - Constraints: Capacity limits, Supply limits, Demand requirements
    # - Output: Optimal truck routes to rebalance bikes
    
    # ### 3Ô∏è‚É£ **Visualization & Insights**
    # - Interactive maps showing station status
    # - Analytics charts for demand patterns
    # - AI-generated operational insights with management recommendations
    # - Downloadable CSV reports for operations teams
    
    # ---
    # """)
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("## ‚öôÔ∏è Simulation Controls")
            
            with gr.Accordion("üìÖ Date & Time Settings", open=True):
                selected_date = gr.Textbox(
                    label="üìÖ Date (YYYY-MM-DD)",
                    value="2024-08-15",
                    placeholder="YYYY-MM-DD",
                    info="Select any date to simulate"
                )
                
                selected_time = gr.Dropdown(
                    label="üïê Hour of Day",
                    choices=[f"{h:02d}:00" for h in range(24)],
                    value="08:00",
                    info="Peak hours: 7-9 AM, 5-7 PM on weekdays"
                )
            
            with gr.Accordion("üö≤ Bike Availability", open=True):
                # *** CHANGE 4: Updated Slider component ***
                total_bikes = gr.Slider(
                    minimum=0,
                    maximum=30000,
                    value=13695,  # Default to 15 bikes/station (15 * 913)
                    step=100,
                    label="üö≤ Total Bikes in System",
                    info="Set the total bike availability across all 913 stations"
                )
            
            with gr.Accordion("üå¶Ô∏è Weather Conditions", open=True):
                gr.Markdown("""
                **Adjust weather parameters to see their impact on bike demand:**
                - ‚òÄÔ∏è Higher temperatures ‚Üí More demand
                - üåßÔ∏è Rain/precipitation ‚Üí Less demand  
                - üí® High wind ‚Üí Less demand
                - ‚òÅÔ∏è Cloud cover ‚Üí Moderate impact
                """)
                
                with gr.Row():
                    temp = gr.Slider(0, 40, value=20, step=0.5, label="üå°Ô∏è Temperature (¬∞C)")
                    feelslike = gr.Slider(0, 40, value=20, step=0.5, label="ü§î Feels Like (¬∞C)")
                
                with gr.Row():
                    humidity = gr.Slider(0, 100, value=60, step=1, label="üíß Humidity (%)")
                    windspeed = gr.Slider(0, 50, value=10, step=0.5, label="üí® Wind Speed (km/h)")
                
                with gr.Row():
                    precip = gr.Slider(0, 20, value=0, step=0.1, label="üåßÔ∏è Precipitation (mm)")
                    cloudcover = gr.Slider(0, 100, value=50, step=1, label="‚òÅÔ∏è Cloud Cover (%)")
                
                with gr.Row():
                    solarradiation = gr.Slider(0, 1000, value=200, step=10, label="‚òÄÔ∏è Solar Radiation (W/m¬≤)")
                    solarenergy = gr.Slider(0, 10, value=5, step=0.1, label="‚ö° Solar Energy (MJ/m¬≤)")
                
                uvindex = gr.Slider(0, 12, value=5, step=1, label="üîÜ UV Index")
            
            run_btn = gr.Button(
                "üöÄ Run Simulation",
                variant="primary",
                size="lg"
            )
            
            gr.Markdown("---")
            
            gr.Markdown("""
            ### üí° Quick Usage Tips
            
            **Best Times to Test:**
            - üåÖ **Morning Rush:** 7-9 AM (weekdays) - High commuter demand
            - üåÜ **Evening Rush:** 5-7 PM (weekdays) - Return commutes
            - üèñÔ∏è **Weekend:** 1-4 PM (Sat/Sun) - Leisure demand
            - üåô **Late Night:** 11 PM-1 AM - Low demand
            
            **Weather Impact Examples:**
            - ‚òÄÔ∏è Sunny day (temp 25¬∞C, no rain) ‚Üí High demand
            - üåßÔ∏è Rainy day (precip 5mm+) ‚Üí Low demand
            - ü•∂ Cold winter (temp <5¬∞C) ‚Üí Very low demand
            - üå°Ô∏è Extreme heat (temp >35¬∞C) ‚Üí Moderate demand
            
            """)
        
        with gr.Column(scale=2):
            gr.Markdown("## üìä Simulation Results")
            summary_output = gr.Markdown(label="Summary")
    
    gr.Markdown("---")
    
    with gr.Row():
        with gr.Column():
            gr.Markdown("""
            ## üí° Insights
            
            **What you'll see here:**
            - üìä Demand overview (total bikes needed)
            - üöâ Station status breakdown (deficit/surplus/balanced)
            - üéØ Busiest clusters identification
            - üöö Rebalancing recommendations with management suggestions
            - üî• Critical moves that need immediate attention
            """)
            insights_output = gr.Markdown()
    
    gr.Markdown("---")
    gr.Markdown("""
    ## üó∫Ô∏è Interactive Station Status Map
    
    **Station Status Map:** Shows current bike availability vs predicted demand
    - üî¥ **Red circles** = Critical deficit (need bikes ASAP)
    - üü† **Orange circles** = Deficit (need bikes)
    - üîµ **Blue circles** = Balanced (good condition)
    - üü¢ **Green circles** = Surplus (extra bikes available)
    - *Circle size indicates severity of imbalance*
    - *Click on markers for detailed information*
    """)
    
    pred_map_output = gr.HTML(label="Station Status Map", elem_classes="map-container")
    
    gr.Markdown("---")
    gr.Markdown("""
    ## üìà Analytics Dashboard
    
    **Three analytical views to understand patterns:**
    
    1. **Demand by Cluster** - Which station types need most bikes
    2. **Balance Distribution** - How well-balanced is the system
    3. **Move Analysis** - Relationship between distance and bikes moved
    """)
    
    with gr.Row():
        plot1_output = gr.Plot(label="Demand by Cluster")
        plot2_output = gr.Plot(label="Balance Distribution")
    
    plot3_output = gr.Plot(label="Rebalancing Move Analysis")
    
    gr.Markdown("---")
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("""
            ## üì• Download Rebalancing Plan
            
            **CSV file contains:**
            - All moves with station names
            - Number of bikes to move
            - Distances and costs
            - GPS coordinates for navigation
            - Priority information
            
            **Use this for:**
            - Operations planning
            - Truck route optimization
            - Cost estimation
            - Performance tracking
            """)
        with gr.Column(scale=1):
            csv_output = gr.File(label="Complete CSV Report")
    
    # *** CHANGE 5: Updated input list for the click event ***
    run_btn.click(
        fn=run_simulation,
        inputs=[
            selected_date, selected_time, total_bikes,
            temp, feelslike, humidity, windspeed,
            precip, cloudcover, solarradiation, solarenergy, uvindex
        ],
        outputs=[
            pred_map_output,
            plot1_output,
            plot2_output,
            plot3_output,
            insights_output,
            summary_output,
            csv_output
        ]
    )
    
    gr.Markdown("---")
    
    # Pre-configured examples
    gr.Markdown("""
    ## üìç Try These Pre-Configured Scenarios
    
    Click any scenario below to automatically fill in the parameters:
    """)
    
    # *** CHANGE 6: Updated examples with total bike counts ***
    gr.Examples(
        examples=[
            ["2024-08-15", "08:00", 13695, 22, 22, 65, 8, 0, 30, 300, 6, 6],  # Nice morning (15/station)
            ["2024-08-15", "17:00", 13695, 28, 30, 70, 12, 0, 40, 250, 5, 7],  # Warm evening (15/station)
            ["2024-08-17", "14:00", 13695, 25, 26, 60, 10, 0, 50, 400, 7, 8],  # Weekend afternoon (15/station)
            ["2024-08-15", "23:00", 13695, 18, 17, 75, 5, 0, 10, 0, 0, 0],  # Late night (15/station)
            ["2024-12-15", "08:00", 13695, 5, 3, 80, 15, 2, 90, 50, 1, 1],  # Cold/rainy winter (15/station)
            ["2024-08-15", "08:00", 9130, 22, 22, 65, 8, 0, 30, 300, 6, 6],  # Low availability (10/station)
            ["2024-08-15", "08:00", 18260, 22, 22, 65, 8, 0, 30, 300, 6, 6],  # High availability (20/station)
        ],
        inputs=[
            selected_date, selected_time, total_bikes,
            temp, feelslike, humidity, windspeed,
            precip, cloudcover, solarradiation, solarenergy, uvindex
        ],
        label="Scenario Examples",
        examples_per_page=7,
        run_on_click=False
    )
    
    gr.Markdown("---")
    
    # Technical documentation
    with gr.Accordion("üìö Technical Details & Performance Metrics", open=False):
        gr.Markdown("""
        ### üî¨ System Architecture
        
        **Machine Learning Pipeline:**
        1. **Clustering Phase (K-Means)**
           - Grouped 800+ stations into 5 clusters based on usage patterns
           - Features: Morning/evening peaks, weekday/weekend ratios, total volume
           - Clusters represent: Commuter Hubs, Leisure Stations, Transit Centers, etc.
        
        2. **Prediction Phase (XGBoost)**
           - 5 separate XGBoost models (one per cluster)
           - Each model specialized for its cluster's demand patterns
           - Input features: 10 time features + 9 weather features
           - Cyclical encoding for temporal features (sin/cos transformations)
        
        3. **Optimization Phase (MILP)**
           - Mixed Integer Linear Programming using PuLP
           - Objective: Minimize total cost (transport + shortages)
           - Constraints: Capacity, supply, demand, physical limits
           - Solver: CBC (Coin-or branch and cut)
        
        ### üìä Performance Metrics
        
        **Prediction Accuracy:**
        - Average MAE: 0.5-2.5 bikes per hour per station
        - Average RMSE: 0.5-3.0 bikes per hour
        - Average R¬≤: 0.04-0.4
        - Best cluster R¬≤: 0.33 (Commuter hubs - very predictable)
        
        **Optimization Performance:**
        - Average solve time: <1 minute for 700+ stations
        - Typical moves: 50-150 relocations per scenario
        - Cost reduction: 30-50% vs naive rebalancing
        - Shortage reduction: 80-95% deficit stations eliminated
        
        ### üõ†Ô∏è Technology Stack
        
        **Core Libraries:**
        - `pandas` (2.0.3) - Data manipulation
        - `numpy` (1.24.3) - Numerical computations
        - `scikit-learn` (1.3.0) - Clustering algorithms
        - `xgboost` (2.0.3) - Gradient boosting models
        - `pulp` (2.7.0) - Linear programming solver
        
        **Visualization:**
        - `folium` (0.15.1) - Interactive maps
        - `plotly` (5.18.0) - Analytical charts
        - `gradio` - Web interface
        
        **Utilities:**
        - `haversine` (2.8.0) - Distance calculations
        - `joblib` (1.3.2) - Model serialization
        
        ### üìÅ File Structure
        
        ```
        project/
        ‚îú‚îÄ‚îÄ app.py                     # Main dashboard code
        ‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
        ‚îú‚îÄ‚îÄ data/                      # Data folder
        ‚îÇ   ‚îú‚îÄ‚îÄ station_locations.csv    # GPS coordinates
        ‚îÇ   ‚îú‚îÄ‚îÄ station_capacity.csv     # Dock capacities
        ‚îÇ   ‚îú‚îÄ‚îÄ station_clusters.csv     # Cluster assignments
        ‚îÇ   ‚îî‚îÄ‚îÄ distance_matrix.npy      # Pre-computed distances
        ‚îî‚îÄ‚îÄ models/                    # Model folder
            ‚îú‚îÄ‚îÄ model_cluster_0.pkl    # Cluster 0 XGBoost model
            ‚îú‚îÄ‚îÄ model_cluster_1.pkl    # Cluster 1 XGBoost model
            ‚îú‚îÄ‚îÄ model_cluster_2.pkl    # Cluster 2 XGBoost model
            ‚îú‚îÄ‚îÄ model_cluster_3.pkl    # Cluster 3 XGBoost model
            ‚îî‚îÄ‚îÄ model_cluster_4.pkl    # Cluster 4 XGBoost model
        ```
        
        ### üéØ Business Impact
        
        **Cost Savings:**
        - Reduced truck miles: ~30% fewer kilometers
        - Optimized labor: Better route planning
        - Fewer shortages: Improved customer satisfaction
        
        **Operational Benefits:**
        - Real-time decision support
        - Proactive vs reactive operations
        - Data-driven resource allocation
        - Scalable to any bike-share system
        
        ### üìñ Data Sources
        
        **Training Data:**
        - Capital Bikeshare trip data (2020-2024)
        - 50+ million trips analyzed
        - Weather data from Visual Crossing API
        - Station metadata from official feeds
        
        **Features Engineered:**
        - Temporal: hour, day of week, month, holidays
        - Cyclical: sin/cos transformations for time
        - Weather: temperature, humidity, precipitation, wind, solar
        - Station: cluster, capacity, location
        - Behavioral: weekday/weekend patterns, peak hour indicators
        """)
    
    with gr.Accordion("‚ùì Frequently Asked Questions", open=False):
        gr.Markdown("""
        ### General Questions
        
        **Q: Can I use this for other cities?**
        A: Yes! The methodology is transferable. You'd need to retrain models on local data (trip history, weather, station info).
        
        **Q: How often should rebalancing be done?**
        A: Typically 2-3 times daily during peak hours (morning, evening, and optionally midday). This dashboard helps identify when and where.
        
        **Q: What if the weather changes suddenly?**
        A: Update the weather parameters and re-run. The model adjusts predictions based on current conditions.
        
        **Q: What does "manageable without movement" mean?**
        A: When there are only minor imbalances (‚â§10 moves, ‚â§50 bikes total), the system can function adequately without immediate rebalancing.
        
        ### Technical Questions
        
        **Q: Why use cluster-based models instead of one global model?**
        A: Different stations have different usage patterns. A commuter station behaves differently than a tourist area. Cluster-specific models capture these nuances better.
        
        **Q: How is the distance matrix calculated?**
        A: Using the Haversine formula on GPS coordinates, which calculates great-circle distances between station pairs. This gives accurate "as-the-crow-flies" distances.
        
        **Q: What optimization algorithm is used?**
        A: Mixed Integer Linear Programming (MILP) solved with CBC (Coin-or Branch and Cut). It guarantees optimal solutions for this problem size.
        
        **Q: Can the system handle real-time data?**
        A: Yes, if integrated with live APIs. Currently uses user-specified bike levels, but can easily connect to real-time station status feeds.
        
        ### Operations Questions
        
        **Q: How should operations teams use this?**
        A: 1) Set current bike availability, 2) Run predictions for upcoming hours, 3) Review rebalancing recommendations, 4) Assign trucks to routes, 5) Download CSV for drivers.
        
        **Q: What do the colors mean on the maps?**
        A: Red = needs bikes urgently, Orange = needs bikes, Blue = balanced, Green = has extra bikes to share.
        
        **Q: How are costs calculated?**
        A: Transport cost = $0.50 per kilometer per bike. Shortage penalty = $10 per unmet bike. These are configurable parameters.
        
        **Q: What if a truck has limited capacity?**
        A: The current model assumes unlimited truck capacity. For real operations, you'd add truck capacity constraints to the optimization problem.
        """)
    
    gr.Markdown("""
    ---
    
    ### üë®‚Äçüíª About This Project
    
    **Developed as a comprehensive machine learning and operations research project demonstrating:**
    - End-to-end ML pipeline (data ‚Üí models ‚Üí deployment)
    - Integration of multiple techniques (clustering, regression, optimization)
    - Real-world business application with measurable impact
    - Production-ready dashboard with intuitive UX
    
    **Project Phases:**
    1. Data preparation & feature engineering
    2. Unsupervised learning (K-Means clustering)
    3. Supervised learning (XGBoost prediction models)
    4. Optimization research (MILP optimization)
    5. Deployment (Interactive Gradio dashboard)
    
    **Dataset:** Capital Bikeshare System Data (Washington D.C.) - 2020-2024
    
    **Impact:** Helps optimize bike distribution, reduce operational costs, and improve customer experience for 800+ stations serving 5+ million annual trips.
    
    ---
    
    Built with ‚ù§Ô∏è for smarter urban mobility | ML Project
    
    Built By:
    1. Lakshhmi Narayanan T - 23BAI1048 - VIT Chennai
    2. Vijay Aravindh S - 23BAI1160 - VIT Chennai
    3. Mohammed Farhan Rashid - 23BAI1529 - VIT Chennai
    """)

# Launch the application
if __name__ == "__main__":
    demo.launch(
        share=True,
        server_name="0.0.0.0",
        server_port=7860
    )